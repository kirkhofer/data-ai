{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize environment\n",
    "> NOTE: Make sure you have a .env file loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Get Configuration Settings\n",
    "load_dotenv()\n",
    "\n",
    "client = AzureOpenAI(api_key=os.getenv('AOAI_KEY'), azure_endpoint=os.getenv('AOAI_ENDPOINT'), api_version=os.environ.get(\"AOAI_VERSION\",\"2023-03-15-preview\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-8irqquY2tNjGPhCNZQ8QIRfKN3VgB', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Garbanzo beans and chickpeas are actually the same thing - they are just known by different names in different parts of the world. In the United States, they are typically referred to as chickpeas, while in many other countries such as Mexico and Spain, they are called garbanzo beans. So whether you call them chickpeas or garbanzo beans, you're talking about the same type of legume!\", role='assistant', function_call=None, tool_calls=None))], created=1705704128, model='gpt-35-turbo', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=87, prompt_tokens=38, total_tokens=125))\n",
      "Garbanzo beans and chickpeas are actually the same thing - they are just known by different names in different parts of the world. In the United States, they are typically referred to as chickpeas, while in many other countries such as Mexico and Spain, they are called garbanzo beans. So whether you call them chickpeas or garbanzo beans, you're talking about the same type of legume!\n"
     ]
    }
   ],
   "source": [
    "#https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/chatgpt?pivots=programming-language-chat-completions\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-35-turbo\", # The deployment name you chose when you deployed the ChatGPT or GPT-4 model.\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What's the difference between garbanzo beans and chickpeas?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response)\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example from the documentation around Chat GPT and using tokens from [here](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/chatgpt?pivots=programming-language-chat-completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Why did the tomato turn red? Because it saw the salad dressing!\n",
      "\n",
      "\n",
      "Why did the cookie go to the doctor? Because it was feeling crummy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conversation=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "while(True):\n",
    "    user_input = input()      \n",
    "    if user_input == \"exit\" or user_input == \"quit\" or user_input == \"\":\n",
    "        break\n",
    "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-35-turbo\", # The deployment name you chose when you deployed the ChatGPT or GPT-4 model.\n",
    "        messages = conversation\n",
    "    )\n",
    "\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
    "    print(\"\\n\" + response.choices[0].message.content + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "max_response_tokens = 250\n",
    "token_limit= 4096\n",
    "conversation=[]\n",
    "conversation.append(system_message)\n",
    "\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":  # if there's a name, the role is omitted\n",
    "                num_tokens += -1  # role is always required and always 1 token\n",
    "    num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "    return num_tokens\n",
    "\n",
    "while(True):\n",
    "    user_input = input(\"\")\n",
    "    if user_input == \"exit\" or user_input == \"quit\" or user_input == \"\":\n",
    "        break\n",
    "             \n",
    "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "    conv_history_tokens = num_tokens_from_messages(conversation)\n",
    "\n",
    "    while (conv_history_tokens+max_response_tokens >= token_limit):\n",
    "        del conversation[1] \n",
    "        conv_history_tokens = num_tokens_from_messages(conversation)\n",
    "        \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-35-turbo\", # The deployment name you chose when you deployed the ChatGPT or GPT-4 model.\n",
    "        messages = conversation,\n",
    "        temperature=.7,\n",
    "        max_tokens=max_response_tokens,\n",
    "    )\n",
    "\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
    "    print(\"\\n\" + response.choices[0].message.content + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bot for reading content from a web site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=input(\"Enter a url for a web site\")\n",
    "conversation=[{\"role\": \"system\", \"content\": f\"You are a helpful assistant that knows alot about {url}. Read the webside and answer questions.\"}]\n",
    "\n",
    "while(True):\n",
    "    user_input = input()      \n",
    "    if user_input == \"exit\" or user_input == \"quit\" or user_input == \"\":\n",
    "        break\n",
    "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-35-turbo\", # The deployment name you chose when you deployed the ChatGPT or GPT-4 model.\n",
    "        messages = conversation\n",
    "    )\n",
    "\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
    "    print(\"\\n\" + response.choices[0].message.content + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"The CSS code for a color like a blue sky at dusk:\n",
    "\n",
    "background-color: #\"\"\"\n",
    "response=client.completions.create(prompt=prompt,\n",
    "    model='text-davinci-003',\n",
    "    max_tokens=64, \n",
    "    temperature=0, \n",
    "    top_p=1, \n",
    "    frequency_penalty=0.0, \n",
    "    presence_penalty=0.0, \n",
    "    stop=[\";\"])\n",
    "\n",
    "print(f\"{response.choices[0].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"Write a tagline for an ice cream shop.\"\n",
    "response=client.completions.create(\n",
    "    prompt=prompt,\n",
    "    mocel='text-davinci-003'\n",
    ")\n",
    "\n",
    "print(f\"{response.choices[0].text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"\n",
    "The following is a list of companies and the categories they fall into:\n",
    "\n",
    "Apple, Facebook, Fedex\n",
    "\n",
    "Apple\n",
    "Category:\n",
    "\"\"\"\n",
    "\n",
    "response = client.completions.create(\n",
    "  model='text-davinci-003',\n",
    "  prompt=prompt,\n",
    "  temperature=0,\n",
    "  max_tokens=64,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0\n",
    ")\n",
    "\n",
    "print(f\"{response.choices[0].text}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All about SQL Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'openai' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124m### Postgres SQL tables, with their properties:\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m#\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124mSELECT\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241m.\u001b[39mCompletion\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     13\u001b[0m   engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcode-davinci-002\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m   prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m     15\u001b[0m   temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     16\u001b[0m   max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m,\n\u001b[1;32m     17\u001b[0m   top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m     18\u001b[0m   frequency_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m     19\u001b[0m   presence_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m     20\u001b[0m     stop\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'openai' is not defined"
     ]
    }
   ],
   "source": [
    "prompt=\"\"\"\n",
    "### Postgres SQL tables, with their properties:\n",
    "#\n",
    "# Employee(id, name, department_id)\n",
    "# Department(id, name, address)\n",
    "# Salary_Payments(id, employee_id, amount, date)\n",
    "#\n",
    "#Create a SQL query for A query to list the names of the departments which employed more than 10 employees in the last 3 months\n",
    "SELECT\n",
    "\"\"\"\n",
    "\n",
    "response = client.completions.create(\n",
    "  model='code-davinci-002',\n",
    "  prompt=prompt,\n",
    "  temperature=0,\n",
    "  max_tokens=150,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "    stop=[\"#\",\";\"]\n",
    ")\n",
    "\n",
    "print(f\"{response.choices[0].text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bot to write SQL for multiple questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same thing but will ONLY generate the SQL statement\n",
    "prompt=\"\"\"\n",
    "Postgres SQL tables, with their properties:\n",
    "Employee(id, name, department_id)\n",
    "Department(id, name, address)\n",
    "Salary_Payments(id, employee_id, amount, date)\n",
    "\n",
    "#Create a SQL query for \"{0}\"\n",
    "\"\"\"\n",
    "\n",
    "while(True):\n",
    "    user_input = input()      \n",
    "    if user_input == \"exit\" or user_input == \"quit\" or user_input == \"\":\n",
    "        break\n",
    "\n",
    "    sql = prompt.format(user_input)\n",
    "\n",
    "    response = client.completions.create(\n",
    "    model='code-davinci-002',\n",
    "    prompt=sql,\n",
    "    temperature=0,\n",
    "    max_tokens=150,\n",
    "    top_p=1.0,\n",
    "    frequency_penalty=0.0,\n",
    "    presence_penalty=0.0,\n",
    "      stop=[\"#\",\";\"]\n",
    "    )\n",
    "\n",
    "    print(f\"{response.choices[0].text}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This using GPT model and gets more conversational\n",
    "Do you like the conversation? You can easily parse the SQL returned with LangChain or other coding to then turnaround and execute the SQL for the end users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat SQL with ChatGPT\n",
    "prompt=\"\"\"\n",
    "Tables and columns:\n",
    "Employee(id, name, department_id)\n",
    "Department(id, name, address)\n",
    "Salary_Payments(id, employee_id, amount, date)\n",
    "\n",
    "You are a bot that generates SQL code based on the \"Tables and columns\". Return standard SQL queries that a user could then run in a tool like SQL Server Management Studio\n",
    "\"\"\"\n",
    "\n",
    "conversation=[{\"role\": \"system\", \"content\": prompt}]\n",
    "\n",
    "while(True):\n",
    "    user_input = input()      \n",
    "    if user_input == \"exit\" or user_input == \"quit\" or user_input == \"\":\n",
    "        break\n",
    "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-35-turbo\", # The deployment name you chose when you deployed the ChatGPT or GPT-4 model.\n",
    "        messages = conversation\n",
    "    )\n",
    "\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
    "    print(\"\\n\" + response.choices[0].message.content + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "while(True):\n",
    "    user_input = input()      \n",
    "    if user_input == \"exit\" or user_input == \"quit\" or user_input == \"\":\n",
    "        break\n",
    "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-35-turbo\", # The deployment name you chose when you deployed the ChatGPT or GPT-4 model.\n",
    "        messages = conversation\n",
    "    )\n",
    "\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
    "    print(\"\\n\" + response.choices[0].message.content + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ¨ðŸŒŒâ­ï¸ðŸ’«\n"
     ]
    }
   ],
   "source": [
    "prompt=\"\"\"\n",
    "Convert movie titles into emoji.\n",
    "\n",
    "Back to the Future: ðŸ‘¨ðŸ‘´ðŸš—ðŸ•’ \n",
    "Batman: ðŸ¤µðŸ¦‡ \n",
    "Transformers: ðŸš—ðŸ¤– \n",
    "Star Wars:\n",
    "\"\"\"\n",
    "\n",
    "response = client.completions.create(\n",
    "  model='text-davinci-003',\n",
    "  prompt=prompt,\n",
    "  temperature=0.8,\n",
    "  max_tokens=60,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"\\n\"]\n",
    ")\n",
    "\n",
    "print(f\"{response.choices[0].text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"\n",
    "class Log:\n",
    "    def __init__(self, path):\n",
    "        dirname = os.path.dirname(path)\n",
    "        os.makedirs(dirname, exist_ok=True)\n",
    "        f = open(path, \"a+\")\n",
    "\n",
    "        # Check that the file is newline-terminated\n",
    "        size = os.path.getsize(path)\n",
    "        if size > 0:\n",
    "            f.seek(size - 1)\n",
    "            end = f.read(1)\n",
    "            if end != \"\\n\":\n",
    "                f.write(\"\\n\")\n",
    "        self.f = f\n",
    "        self.path = path\n",
    "\n",
    "    def log(self, event):\n",
    "        event[\"_event_id\"] = str(uuid.uuid4())\n",
    "        json.dump(event, self.f)\n",
    "        self.f.write(\"\\n\")\n",
    "\n",
    "    def state(self):\n",
    "        state = {\"complete\": set(), \"last\": None}\n",
    "        for line in open(self.path):\n",
    "            event = json.loads(line)\n",
    "            if event[\"type\"] == \"submit\" and event[\"success\"]:\n",
    "                state[\"complete\"].add(event[\"id\"])\n",
    "                state[\"last\"] = event\n",
    "        return state\n",
    "\n",
    "#\n",
    "Here's what the above class is doing:\n",
    "1.\n",
    "\"\"\"\n",
    "\n",
    "response = client.completions.create(\n",
    "  model='code-davinci-002',\n",
    "  prompt=prompt,\n",
    "  temperature=0,\n",
    "  max_tokens=64,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"#\"]\n",
    ")\n",
    "\n",
    "print(f\"{response.choices[0].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"\n",
    "Q: Who is Batman?\n",
    "A: Batman is a fictional comic book character.\n",
    "\n",
    "Q: What is torsalplexity?\n",
    "A: ?\n",
    "\n",
    "Q: What is Devz9?\n",
    "A: ?\n",
    "\n",
    "Q: Who is George Lucas?\n",
    "A: George Lucas is American film director and producer famous for creating Star Wars.\n",
    "\n",
    "Q: What is the capital of California?\n",
    "A: Sacramento.\n",
    "\n",
    "Q: What orbits the Earth?\n",
    "A: The Moon.\n",
    "\n",
    "Q: Who is Fred Rickerson?\n",
    "A: ?\n",
    "\n",
    "Q: What is an atom?\n",
    "A: An atom is a tiny particle that makes up everything.\n",
    "\n",
    "Q: Who is Alvan Muntz?\n",
    "A: ?\n",
    "\n",
    "Q: What is Kozar-09?\n",
    "A: ?\n",
    "\n",
    "Q: How many moons does Mars have?\n",
    "A: Two, Phobos and Deimos.\n",
    "\n",
    "Q: What's a language model?\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "response = client.completions.create(\n",
    "  model='text-davinci-003',\n",
    "  prompt=prompt,\n",
    "  temperature=0,\n",
    "  max_tokens=60,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0\n",
    ")\n",
    "\n",
    "print(f\"{response.choices[0].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"\n",
    "##### Fix bugs in the below function\n",
    " \n",
    "### Buggy Python\n",
    "import Random\n",
    "a = random.randint(1,12)\n",
    "b = random.randint(1,12)\n",
    "for i in range(10):\n",
    "    question = \"What is \"+a+\" x \"+b+\"? \"\n",
    "    answer = input(question)\n",
    "    if answer = a*b\n",
    "        print (Well done!)\n",
    "    else:\n",
    "        print(\"No.\")\n",
    "    \n",
    "### Fixed Python\n",
    "\"\"\"\n",
    "\n",
    "response = client.completions.create(\n",
    "  model='code-davinci-002',\n",
    "  prompt=prompt,\n",
    "  temperature=0,\n",
    "  max_tokens=182,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"###\"]\n",
    ")\n",
    "\n",
    "print(f\"{response.choices[0].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"\n",
    "You: How do I combine arrays?\n",
    "JavaScript chatbot: You can use the concat() method.\n",
    "You: How do you make an alert appear after 10 seconds?\n",
    "JavaScript chatbot\n",
    "\"\"\"\n",
    "\n",
    "response = client.completions.create(\n",
    "  model='code-davinci-002',\n",
    "  prompt=prompt,\n",
    "  temperature=0,\n",
    "  max_tokens=60,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.5,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"You:\"]\n",
    ")\n",
    "\n",
    "print(f\"{response.choices[0].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"\n",
    "Extract the airport codes from this text:\n",
    "\n",
    "Text: \"I want to fly from Los Angeles to Miami.\"\n",
    "Airport codes: LAX, MIA\n",
    "\n",
    "Text: \"I want to fly from Orlando to Boston\"\n",
    "Airport codes:\n",
    "\"\"\"\n",
    "\n",
    "response = client.completions.create(\n",
    "  model='text-davinci-003',\n",
    "  prompt=prompt,\n",
    "  max_tokens=60,\n",
    "  temperature=0,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"\\n\"]\n",
    ")\n",
    "\n",
    "print(f\"{response.choices[0].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"\n",
    "You: What have you been up to?\n",
    "Friend: Watching old movies.\n",
    "You: Did you watch anything interesting?\n",
    "Friend:\n",
    "\"\"\"\n",
    "\n",
    "response = client.completions.create(\n",
    "    model='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    temperature=0.5,\n",
    "    max_tokens=60,\n",
    "    top_p=1.0,\n",
    "    frequency_penalty=0.5,\n",
    "    presence_penalty=0.0,\n",
    "    stop=[\"You:\"]\n",
    ")\n",
    "\n",
    "print(f\"{response.choices[0].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"\n",
    "Brainstorm some ideas combining VR and fitness:\n",
    "\"\"\"\n",
    "\n",
    "response = client.completions.create(\n",
    "    model='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    temperature=0.6,\n",
    "    max_tokens=150,\n",
    "    top_p=1.0,\n",
    "    frequency_penalty=1,\n",
    "    presence_penalty=1\n",
    ")\n",
    "\n",
    "print(f\"{response.choices[0].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"\n",
    "Create a list of 8 questions for my interview with a science fiction author:\n",
    "\"\"\"\n",
    "\n",
    "response = client.completions.create(\n",
    "    model='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    temperature=0.5,\n",
    "    max_tokens=150,\n",
    "    top_p=1.0,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0\n",
    ")\n",
    "\n",
    "print(f\"{response.choices[0].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"\n",
    "Create an outline for an essay about Nikola Tesla and his contributions to technology:\n",
    "\"\"\"\n",
    "\n",
    "response = client.completions.create(\n",
    "    model='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    temperature=0.3,\n",
    "    max_tokens=150,\n",
    "    top_p=1.0,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0\n",
    ")\n",
    "\n",
    "print(f\"{response.choices[0].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"\n",
    "What are 5 key points I should know when studying Ancient Rome?\n",
    "\"\"\"\n",
    "\n",
    "response = client.completions.create(\n",
    "    model='text-davinci-003',\n",
    "    prompt=prompt,\n",
    "    temperature=0.3,\n",
    "    max_tokens=150,\n",
    "    top_p=1.0,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0\n",
    ")\n",
    "\n",
    "print(f\"{response.choices[0].text}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example from the documentation around Chat GPT and using tokens from [here](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/chatgpt?pivots=programming-language-chat-completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "system_message = {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "max_response_tokens = 250\n",
    "token_limit= 4096\n",
    "conversation=[]\n",
    "conversation.append(system_message)\n",
    "\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":  # if there's a name, the role is omitted\n",
    "                num_tokens += -1  # role is always required and always 1 token\n",
    "    num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "    return num_tokens\n",
    "\n",
    "while(True):\n",
    "    user_input = input(\"\")\n",
    "    if user_input == \"exit\" or user_input == \"quit\" or user_input == \"\":\n",
    "        break\n",
    "             \n",
    "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "    conv_history_tokens = num_tokens_from_messages(conversation)\n",
    "\n",
    "    while (conv_history_tokens+max_response_tokens >= token_limit):\n",
    "        del conversation[1] \n",
    "        conv_history_tokens = num_tokens_from_messages(conversation)\n",
    "        \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-35-turbo\", # The deployment name you chose when you deployed the ChatGPT or GPT-4 model.\n",
    "        messages = conversation,\n",
    "        temperature=.7,\n",
    "        max_tokens=max_response_tokens,\n",
    "    )\n",
    "\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
    "    print(\"\\n\" + response.choices[0].message.content + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai102",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
