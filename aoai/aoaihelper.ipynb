{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure OpenAI Helper\n",
    "We here a lot about having rate limits on subscriptions and different models. While you can implement that in appliances in front of your resources in Azure you can also build it into a simple yet reliable class to \"retry\" on a \"429\" error. This notebook implements a `@retry` from the `tenacity` library to retry on a 429 error and automatically swtiches to your other deployments for you.\n",
    "\n",
    "## Setup for one to many endpoints\n",
    "1. At minimum create the environment variables\n",
    "    - Set the `OPENAI_API_BASE` and the `OPENAI_API_KEY` environment variables at minimum\n",
    "1. OR: Create a json.env file with the content like below\n",
    "    ```json\n",
    "    [\n",
    "        {\"endpoint\":\"https://openairesource1.openai.azure.com/\",\"key\":\"999aaa9999\"},\n",
    "        {\"endpoint\":\"https://openairesource2.openai.azure.com/\",\"key\":\"999aaa9999\"}\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "## Use the AOAIHelper class as defined in this document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt, retry_if_exception_type,stop_after_delay\n",
    "\n",
    "# Get Configuration Settings from a .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'endpoint': 'https://aoai-dai-dev-use.openai.azure.com/', 'model': 'gpt-35-turbo', 'deployment': 'gpt-35-turbo'}, {'endpoint': 'https://aoai-dai-dev-use.openai.azure.com/', 'model': 'code-davinci-002', 'deployment': 'code-davinci-002'}, {'endpoint': 'https://aoai-dai-dev-use.openai.azure.com/', 'model': 'text-davinci-003', 'deployment': 'text-davinci-003'}, {'endpoint': 'https://aoai-dai-dev-use.openai.azure.com/', 'model': 'text-embedding-ada-002', 'deployment': 'text-embedding-ada-002'}, {'endpoint': 'https://aoai-dai-dev-scus.openai.azure.com/', 'model': 'gpt-35-turbo', 'deployment': 'gpt-35-turbo'}, {'endpoint': 'https://aoai-dai-dev-scus.openai.azure.com/', 'model': 'text-embedding-ada-002', 'deployment': 'text-embedding-ada-002'}, {'endpoint': 'https://aoai-dai-dev-scus.openai.azure.com/', 'model': 'text-davinci-003', 'deployment': 'davinci-003'}]\n",
      "end=https://aoai-dai-dev-use.openai.azure.com/\n",
      "request.status_code=200\n",
      "1536\n"
     ]
    }
   ],
   "source": [
    "class AOAIHelper:\n",
    "    __endpoints=[]\n",
    "    __pointers=[]\n",
    "    __models=[]\n",
    "\n",
    "    class OpenAI429Exception(Exception):\n",
    "        \"\"\"Raised when the status code is 429\"\"\"\n",
    "        pass\n",
    "\n",
    "    def __init__(self,endpoints=[]):\n",
    "        if len(endpoints)==0:\n",
    "            endpoints.append({\"endpoint\":os.getenv('OPENAI_API_BASE'),\"key\":os.getenv('OPENAI_API_KEY')})\n",
    "        \n",
    "        self.__endpoints=endpoints\n",
    "\n",
    "        for endpoint in endpoints:\n",
    "            headers={\"Content-Type\":\"application/json\",\"api-key\":endpoint['key']}\n",
    "            uri = f\"{endpoint['endpoint']}openai/deployments?api-version=2022-12-01\"\n",
    "\n",
    "            response = requests.get(uri,headers=headers).json()\n",
    "            for dep in response['data']:\n",
    "                self.__models.append({\"endpoint\":endpoint['endpoint'],\"key\":endpoint['key'],\"model\":dep['model'],\"deployment\":dep['id']})\n",
    "\n",
    "        # Get distinct modelNames in the endpoints list\n",
    "        pts = list(set([x['model'] for x in self.__models]))\n",
    "        for pt in pts:\n",
    "            x=len([x for x in self.__models if x['model']==pt])\n",
    "            self.__pointers.append({\"model\":pt,\"pointer\":0,\"count\":x})\n",
    "\n",
    "    def __getModel(self,model=\"text-davinci-003\"):\n",
    "        pointer=next((m for m in self.__pointers if m['model'] == model), None) \n",
    "        mods=list(filter(lambda x: x['model'] == model, self.__models))\n",
    "        return mods[pointer['pointer']]\n",
    "    \n",
    "    def __incrementModel(self,model=\"text-davinci-003\"):\n",
    "        pointer=next((m for m in self.__pointers if m['model'] == model), None) \n",
    "        key = pointer['pointer']\n",
    "        if key < pointer['count']-1:\n",
    "            key+=1\n",
    "        else:\n",
    "            key=0\n",
    "        pointer['pointer']=key\n",
    "\n",
    "    @retry(retry=retry_if_exception_type(OpenAI429Exception),wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6) | stop_after_delay(5))\n",
    "    def __request(self,uri,body,model):\n",
    "        point=self.__getModel(model)\n",
    "        key=point[\"key\"]\n",
    "        end=point[\"endpoint\"]\n",
    "        deployment=point[\"deployment\"]\n",
    "\n",
    "        headers={\"Content-Type\":\"application/json\",\"api-key\":key}\n",
    "        uri = uri.format(end=end,deployment=deployment)\n",
    "\n",
    "        request = requests.post(uri, headers=headers, json=body)\n",
    "        print(f\"end={end}\")\n",
    "        print(f\"request.status_code={request.status_code}\")\n",
    "\n",
    "        if request.status_code == 429:\n",
    "            # move to the previous endpoint in the endpoints array\n",
    "            self.__incrementModel(model)\n",
    "            raise self.OpenAI429Exception(\"OpenAI API rate limit exceeded. Retrying...\")\n",
    " \n",
    "        response = request.json()\n",
    "        return response\n",
    "\n",
    "    def Embedding(self,text,model=\"text-embedding-ada-002\"):\n",
    "        uri = \"{end}/openai/deployments/{deployment}/embeddings?api-version=2022-12-01\"    \n",
    "        response = self.__request(uri,{\"input\":text},model)\n",
    "        embeddings = response['data'][0]['embedding']\n",
    "        return embeddings\n",
    "\n",
    "    def Completion(self,prompt, model=\"text-davinci-002\", max_tokens=150, temperature=0.9):\n",
    "        uri = \"{end}openai/deployments/{deployment}/completions?api-version=2022-12-01\"\n",
    "\n",
    "        body={\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\":max_tokens,\n",
    "            \"temperature\":temperature,\n",
    "            \"stop\":[\"#\"]\n",
    "        }\n",
    "\n",
    "        #convert body to utf8 bytes\n",
    "        # body_utf8 = bytes(json.dumps(body), 'utf-8')\n",
    "        response = self.__request(uri,body,model)\n",
    "        if( \"error\" in response ):\n",
    "            #Read this from this json {\"error\":{\"message\":\"blah\"}}\n",
    "            return response['error']['message']\n",
    "        else:\n",
    "            return response['choices'][0]['text']\n",
    "\n",
    "    def Chat(self,messages, model=\"gpt-35-turbo\", max_tokens=150, temperature=0.9):\n",
    "        uri=\"{end}/openai/deployments/{deployment}/chat/completions?api-version=2023-05-15\"\n",
    "\n",
    "        body={\n",
    "            \"messages\": messages,\n",
    "            \"max_tokens\":max_tokens,\n",
    "            \"temperature\":temperature\n",
    "        }\n",
    "\n",
    "        #convert body to utf8 bytes\n",
    "        #body_utf8 = bytes(json.dumps(body), 'utf-8')\n",
    "        response = self.__request(uri,body,model)\n",
    "        if( \"error\" in response ):\n",
    "            #Read this from this json {\"error\":{\"message\":\"blah\"}}\n",
    "            return response['error']['message']\n",
    "        else:\n",
    "            return response['choices'][0]['message']\n",
    "    \n",
    "    def Models(self):\n",
    "        \"\"\"Returns a list of models and their endpoints\"\"\"\n",
    "        models = []\n",
    "        for model in self.__models:\n",
    "            models.append({\"endpoint\":model[\"endpoint\"],\"model\":model[\"model\"],\"deployment\":model[\"deployment\"]})\n",
    "        return models\n",
    "path=\"json.env\"\n",
    "if os.path.exists(path):\n",
    "    # load json.env from file\n",
    "    with open(path) as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    aoaiCls = AOAIHelper(data)\n",
    "else:\n",
    "    aoaiCls = AOAIHelper()\n",
    "    \n",
    "print(aoaiCls.Models())\n",
    "\n",
    "emb = aoaiCls.Embedding(\"This is a tester message\")\n",
    "print(len(emb))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end=https://aoai-dai-dev-use.openai.azure.com/\n",
      "request.status_code=429\n",
      "end=https://aoai-dai-dev-scus.openai.azure.com/\n",
      "request.status_code=200\n",
      "{'role': 'assistant', 'content': \"Why don't scientists trust atoms? \\nBecause they make up everything.\"}\n"
     ]
    }
   ],
   "source": [
    "# Chat with the AI service and run this over and over to get a 429\n",
    "messages=[]\n",
    "question=\"Tell me a funny joke\"\n",
    "messages.append({\"role\":\"user\",\"content\":question})\n",
    "answer=aoaiCls.Chat(messages)\n",
    "print(answer)\n",
    "\n",
    "# If one resource gets a 429, it will move to the next resource"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai102",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
